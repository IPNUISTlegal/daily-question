# 每日一题

[daily-question ](https://github.com/amusi/daily-question) 涉及但不局限于机器学习、深度学习和计算机视觉等方向

Warning：众人拾柴火焰高，如果大家有看到很好的题目，可以通过提交issue的方式把题目和答案分享出来，互相学习，一起进步



# 题目

**1.【排序题】梯度下降算法的正确步骤是什么？（dcaeb）**

a.计算预测值和真实值之间的误差

b.重复迭代，直至得到网络权重的最佳值

c.把输入传入网络，得到输出值

d.用随机值初始化权重和偏差

e.对每一个产生误差的神经元，调整相应的（权重）值以减小误差


**2.【多选题】小明在训练深度学习模型时，发现训练集误差不断减少，测试集误差不断增大，以下解决方法正确的是：（ACD）**

A. 数据增广

B. 增加网络深度

C. 提前停止训练

D. 添加Dropout


**3.【单选题】以下关于鞍点上的Hessian矩阵的描述哪个是正确的？（D）**

A. 正定矩阵

B. 负定矩阵

C. 半正定矩阵

D. 都不对

答案解析：

鞍点：梯度等于零，在其附近Hessian矩阵有正的和负的特征值，行列式小于0，即是不定的。 

参考：[鞍点](https://blog.csdn.net/baidu_27643275/article/details/79250537)

**4.【单选题】以下几种优化方法中，哪种对超参数最不敏感？（C）**

A. SGD（stochatic gradient descent）

B. BGD（batch gradient descent）

C. Adadetla

D. Momentum

答案解析：

1）SGD受到学习率α影响

2）BGD受到batch规模m影响

3）Adagrad的一大优势时可以避免手动调节学习率，比如设置初始的缺省学习率为0.01，然后就不管它，另其在学习的过程中自己变化。

为了避免削弱单调猛烈下降的减少学习率，Adadelta产生了1。Adadelta限制把历史梯度累积窗口限制到固定的尺寸w，而不是累加所有的梯度平方和

4）Momentum：也受到学习率α的影响

**5.【多选题】为什么正则化能处理过拟合？（ABCD）**

A.惩罚了模型的复杂度，避免模型过度学习训练集，提高泛化能力

B.剃刀原理：如果两个理论都能解释一件事情，那么较为简单的理论往往是正确的

C.正则项降低了每一次系数w更新的步伐，使参数更小，模型更简单

D.贝叶斯学派的观点，认为加入了先验分布（l1拉普拉斯分布，l2高斯分布），减少参数的选择空间

答案解析：

A/C 选项没有问题，只不过C中的"步伐"理解起来并不清晰。B/D选项是有点追本溯源的意思，剃刀原理其实是奥卡姆剃刀原理：更小的权值w，从某种意义上说，表示网络的复杂度更低，对数据的拟合刚刚好；从贝叶斯角度理解，为参数 ω 引入拉普拉斯先验分布的最大似然，相当于给均方误差函数加上L1正则项；为参数 ω引入高斯先验分布的最大似然，相当于给均方误差函数加上L2正则项。


参考：

[正则化为什么能防止过拟合（重点地方标红了）](https://www.cnblogs.com/alexanderkun/p/6922428.html)

[【机器学习】从贝叶斯角度理解正则化缓解过拟合](https://blog.csdn.net/u014433413/article/details/78408983)

**6.【单选题】假设你有5个大小为7x7、边界值（Padding）为0，步长（S）为1的卷积核。此时如果你向这一层传入一个维度为224x224x3的数据，那么神经网络下一层所接收到的数据维度是多少？（B）**

A. 220x220x5

B. 218x218x5

C. 217x217x8

D. 217x217x3

答案解析：

正确答案是B。卷积计算公式：Hout=（Himg+2Padding−Kfilterh）/S + 1；Wout=（Wimg+2Padding−Kfilterw）/S + 1。其中Padding是边界填空值，Kfilterw表示卷积核的宽度，S表示步长。

**7.【单选题】假设我们有一个5层的神经网络，这个神经网络在使用一个4GB显存显卡时需要花费3个小时来完成训练。而在测试过程中，单个数据需要花费2秒的时间。如果我们现在把架构变换一下，当评分是0.2和0.3时，分别在第2层和第4层添加Dropout，那么新架构的测试所用时间会变为多少？（C）**

A. 少于2秒

B. 大于2秒

C. 仍是2秒

D. 说不准

答案解析：正确答案是C。在架构中添加Dropout这一改动仅会影响训练过程，而并不影响测试过程。

**8.【单选题】假定目标变量的类别非常不平衡，即主要类别占据了训练数据的99%，现在你的模型在测试集上表现为99%的准确度。那么下面哪一项表述是正确的？（B）**

A. 准确率适合于衡量不平衡类别问题

B. 精确率和召回率适合于衡量不平衡类别问题

C. 精确率和召回率不适合于衡量不平衡类别问题

D. 上述选项都不对

**9.【单选题】下面哪项操作能实现跟神经网络中Dropout的类似效果？（B）**

A. Boosting

B. Bagging

C. Stacking

D. Mapping

答案解析：

dropout的思想继承自bagging方法。

bagging是一种集成方法（ensemble methods）,可以通过集成来减小泛化误差（generalization error）。 
bagging的最基本的思想是通过分别训练几个不同分类器，最后对测试的样本，每个分类器对其进行投票。在机器学习上这种策略叫model averaging。 我们可以把dropout类比成将许多大的神经网络进行集成的一种bagging方法。 

**10.【单选题】在感知机（Perceptron）中的任务顺序是什么？（D）**

1. 随机初始化感知机权重

2. 去到数据集的下一批（batch）

3. 如果预测值和输出不一致，则调整权重

4. 对一个输入样本，计算输出值

A. 1,2,3,4

B. 4,3,2,1

C. 3,1,2,4

D. 1,4,3,2

**11.【单选题】下列哪项关于模型能力（model capacity）的描述是正确的？（指神经网络模型能拟合复杂函数的能力）（A）**

A. 隐藏层层数增加，模型能力增加

B. Dropout的比例增加，模型能力增加

C. 学习率增加，模型能力增加

D. 都不正确

**12.【单选题】关于Logistic回归和SVM，以下说法错误的是？（B）**

A. Logistic回归可用于预测事件发生概率的大小

B. Logistic回归的目标函数是最小化后验概率

C. SVM的目标的结构风险最小化

D. SVM可以加入正则化项，有效避免模型过拟合

答案解析：

Logistic回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。Logistic仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。A正确 Logit回归的输出就是样本属于正类别的几率，可以计算出概率；C正确. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化. D正确. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。

**13.【单选题】在其他条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题？（D）**

A 增加训练集量 

B 减少神经网络隐藏层节点数 

C 删除稀疏的特征

D SVM算法中使用高斯核/RBF核代替线性核

解析：

一般情况下，越复杂的系统，过拟合的可能性就越高，一般模型相对简单的话泛化能力会更好一点。

B.一般认为，增加隐层数可以降低网络误差（也有文献认为不一定能有效降低），提高精度，但也使网络复杂化，从而增加了网络的训练时间和出现“过拟合”的倾向， svm高斯核函数比线性核函数模型更复杂，容易过拟合

D.径向基(RBF)核函数/高斯核函数的说明,这个核函数可以将原始空间映射到无穷维空间。对于参数 ，如果选的很大，高次特征上的权重实际上衰减得非常快，实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调整参数 ，高斯核实际上具有相当高的灵活性，也是 使用最广泛的核函数 之一。


**14.【多选题】以下哪些选项属于线性分类器准则?（ACD）**

A.感知准则函数

B.贝叶斯分类

C.支持向量机

D.Fisher准则


解析： 
线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。

感知准则函数 ：准则函数以使错分类样本到分界面距离之和最小为原则。其优点是通过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感知器的基础。

支持向量机 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小。（使用核函数可解决非线性问题）

Fisher 准则 ：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。

根据两类样本一般类内密集，类间分离的特点，寻找线性分类器最佳的法线向量方向，使两类样本在该方向上的投影满足类内尽可能密集，类间尽可能分开。这种度量通过类内离散矩阵 Sw 和类间离散矩阵 Sb 实现。

